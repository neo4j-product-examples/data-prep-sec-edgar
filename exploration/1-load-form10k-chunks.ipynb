{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load Form 10k Chunk Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Common data processing\n",
    "import json\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "from progress.bar import Bar\n",
    "\n",
    "# Langchain\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_community.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo4j Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neo4j_version(kg: Neo4jGraph) -> Tuple[int, int, int]:\n",
    "  \"\"\"Return the version of Neo4j as a tuple of ints\"\"\"\n",
    "  version = kg.query(\"CALL dbms.components()\")[0][\"versions\"][0]\n",
    "  if \"aura\" in version:\n",
    "    version_tuple = tuple(map(int, version.split(\"-\")[0].split(\".\"))) + (0,)\n",
    "  else:\n",
    "    version_tuple = tuple(map(int, version.split(\".\")))\n",
    "  return version_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neo4j_create_vector_index(kg: Neo4jGraph, index_name: str, \n",
    "                              for_label: str, on_property: str, \n",
    "                              vector_dimensions: int = 1536,\n",
    "                              similarity_function: str = 'cosine') -> None:\n",
    "  \"\"\"Create a Neo4j index for vector properties\"\"\"\n",
    "  \n",
    "  if neo4j_version(kg) >= (5, 15, 0): # For Neo4j version 5.15.0 and above\n",
    "    kg.query(f\"\"\"CREATE VECTOR INDEX `{index_name}` IF NOT EXISTS\n",
    "      FOR (n:{for_label}) ON (n.{on_property}) \n",
    "      OPTIONS {{indexConfig: {{\n",
    "        `vector.dimensions`: {vector_dimensions},\n",
    "        `vector.similarity_function`: '{similarity_function}'\n",
    "      }}}}\"\"\"\n",
    "    )\n",
    "  else: # For Neo4j versions below 5.15.0 (warning: this is deprecated)\n",
    "    kg.query(\n",
    "      f\"\"\"CALL db.index.vector.createNodeIndex(\n",
    "        '{index_name}', '{for_label}', '{on_property}', \n",
    "        {vector_dimensions}, '{similarity_function}'\n",
    "      )\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Set up Neo4j and Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load from environment\n",
    "load_dotenv('.env', override=True)\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE') or 'neo4j'\n",
    "\n",
    "# Global constants\n",
    "VECTOR_INDEX_NAME = 'form_10k_chunks'\n",
    "VECTOR_NODE_LABEL = 'Chunk'\n",
    "VECTOR_SOURCE_PROPERTY = 'text'\n",
    "VECTOR_EMBEDDING_PROPERTY = 'textEmbedding'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a knowledge graph using Langchain's Neo4j integration.\n",
    "# This will be used for direct querying of the knowledge graph. \n",
    "kg = Neo4jGraph(\n",
    "    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
    ")\n",
    "\n",
    "# OpenAI for creating embeddings\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "# Splitting text into chunks using the RecursiveCharacterTextSplitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector index for the textEmbedding property of Text nodes. Call the index \"form_10k_text\" \n",
    "neo4j_create_vector_index(kg, VECTOR_INDEX_NAME, 'Chunk', 'textEmbedding')\n",
    "\n",
    "# Create a uniqueness constraint on the textId property of Text nodes \n",
    "kg.query('CREATE CONSTRAINT unique_chunk IF NOT EXISTS FOR (n:Chunk) REQUIRE n.chunkId IS UNIQUE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a vector store for the Neo4j knowledge graph\n",
    "# This will be used for vector similarity queries.\n",
    "vector_store = Neo4jVector.from_existing_index(\n",
    "    OpenAIEmbeddings(),\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name=VECTOR_INDEX_NAME,\n",
    ")\n",
    "vector_store = Neo4jVector.from_existing_graph(\n",
    "    embedding=embeddings_model,\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name=VECTOR_INDEX_NAME,\n",
    "    node_label=VECTOR_NODE_LABEL,\n",
    "    text_node_properties=[VECTOR_SOURCE_PROPERTY],\n",
    "    embedding_node_property=VECTOR_EMBEDDING_PROPERTY,\n",
    ")\n",
    "retriever = vector_store.as_retriever()\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    ChatOpenAI(temperature=0), chain_type=\"stuff\", retriever=retriever\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Cypher Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties are the following:\n",
      "\n",
      "Relationship properties are the following:\n",
      "\n",
      "The relationships are the following:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kg.refresh_schema()\n",
    "print(kg.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'message': 'Hello Andreas!'}]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Hello world\" query in cypher. Returns a concatenated string using a parameter.\n",
    "kg.query(\"RETURN 'Hello ' + $name as message\", params={'name':\"Andreas!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a node in the knowledge graph for a person named \"Andreas\"\n",
    "kg.query(\"CREATE (n:Person {name: $name})\", params={'name':\"Andreas\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Andreas'}]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the person node we just created by looking for a single node pattern\n",
    "# that matches any node labeled \"Person\" and return the name property of the first one\n",
    "kg.query(\"MATCH (n:Person) RETURN n.name as name LIMIT 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up by removing that example node\n",
    "kg.query(\"MATCH (n:Person) WHERE n.name=$name DETACH DELETE n\", params={'name':\"Andreas\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'count': 0}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many nodes exist in the database\n",
    "kg.query(\"MATCH (n) RETURN count(n) as count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neo4j_vector_search(kg: Neo4jGraph, embeddings_model: OpenAIEmbeddings,\n",
    "                        index_name: str, query: str, top_k: int = 10) -> List:\n",
    "  \"\"\"Search for similar nodes using the Neo4j vector index\"\"\"\n",
    "  embedded_query = embeddings_model.embed_query(query)\n",
    "  vector_search = f\"\"\"\n",
    "    CALL db.index.vector.queryNodes($index_name, $top_k, $embedding) yield node, score\n",
    "    RETURN node.text AS result\n",
    "  \"\"\"\n",
    "  similar = kg.query(vector_search, params={'embedding': embedded_query, 'index_name':index_name, 'top_k': top_k})\n",
    "  return similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Neo4j indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_map(x):\n",
    "    if type(x) == str:\n",
    "        return x, x\n",
    "    elif type(x) == tuple:\n",
    "        return x\n",
    "    else:\n",
    "        raise Exception(\"Entry must of type string or tuple\")\n",
    "\n",
    "\n",
    "def make_set_clause(prop_names: ArrayLike, element_name='n', item_name='rec'):\n",
    "    clause_list = []\n",
    "    for prop_name in prop_names:\n",
    "        clause_list.append(f'{element_name}.{prop_name} = {item_name}.{prop_name}')\n",
    "    return 'SET ' + ', '.join(clause_list)\n",
    "\n",
    "\n",
    "def make_node_merge_query(node_key_name: str, node_label: str, cols: ArrayLike):\n",
    "    template = f'''UNWIND $recs AS rec\\nMERGE(n:{node_label} {{{node_key_name}: rec.{node_key_name}}})'''\n",
    "    prop_names = [x for x in cols if x != node_key_name]\n",
    "    if len(prop_names) > 0:\n",
    "        template = template + '\\n' + make_set_clause(prop_names)\n",
    "    return template + '\\nRETURN count(n) AS nodeLoadedCount'\n",
    "\n",
    "\n",
    "def make_rel_merge_query(source_target_labels: Union[Tuple[str, str], str],\n",
    "                         source_node_key: Union[Tuple[str, str], str],\n",
    "                         target_node_key: Union[Tuple[str, str], str],\n",
    "                         rel_type: str,\n",
    "                         cols: ArrayLike,\n",
    "                         rel_key: str = None):\n",
    "    source_target_label_map = make_map(source_target_labels)\n",
    "    source_node_key_map = make_map(source_node_key)\n",
    "    target_node_key_map = make_map(target_node_key)\n",
    "\n",
    "    merge_statement = f'MERGE(s)-[r:{rel_type}]->(t)'\n",
    "    if rel_key is not None:\n",
    "        merge_statement = f'MERGE(s)-[r:{rel_type} {{{rel_key}: rec.{rel_key}}}]->(t)'\n",
    "\n",
    "    template = f'''\\tUNWIND $recs AS rec\n",
    "    MATCH(s:{source_target_label_map[0]} {{{source_node_key_map[0]}: rec.{source_node_key_map[1]}}})\n",
    "    MATCH(t:{source_target_label_map[1]} {{{target_node_key_map[0]}: rec.{target_node_key_map[1]}}})\\n\\t''' + merge_statement\n",
    "    prop_names = [x for x in cols if x not in [rel_key, source_node_key_map[1], target_node_key_map[1]]]\n",
    "    if len(prop_names) > 0:\n",
    "        template = template + '\\n\\t' + make_set_clause(prop_names, 'r')\n",
    "    return template + '\\n\\tRETURN count(r) AS relLoadedCount'\n",
    "\n",
    "\n",
    "def batches(xs, n=100):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i + n] for i in range(0, len(xs), n)]\n",
    "\n",
    "\n",
    "def load_nodes(graph: Neo4jGraph, node_df: pd.DataFrame, node_key_col: str, node_label: str, batch_size=1000):\n",
    "    records = node_df.to_dict('records')\n",
    "    print(f'======  loading {node_label} nodes  ======')\n",
    "    total = len(records)\n",
    "    print(f'staging {total:,} records')\n",
    "    query = make_node_merge_query(node_key_col, node_label, node_df.columns.copy())\n",
    "    print(f'\\nUsing This Cypher Query:\\n```\\n{query}\\n```\\n')\n",
    "    cumulative_count = 0\n",
    "    for recs in batches(records, batch_size):\n",
    "        res = graph.query(query, params={'recs': recs})\n",
    "        cumulative_count += res[0].get('nodeLoadedCount')\n",
    "        print(f'Loaded {cumulative_count:,} of {total:,} nodes')\n",
    "\n",
    "\n",
    "def load_rels(graph: Neo4jGraph,\n",
    "              rel_df: pd.DataFrame,\n",
    "              source_target_labels: Union[Tuple[str, str], str],\n",
    "              source_node_key: Union[Tuple[str, str], str],\n",
    "              target_node_key: Union[Tuple[str, str], str],\n",
    "              rel_type: str,\n",
    "              rel_key: str = None,\n",
    "              batch_size=10_000):\n",
    "    records = rel_df.to_dict('records')\n",
    "    print(f'======  loading {rel_type} relationships  ======')\n",
    "    total = len(records)\n",
    "    print(f'staging {total:,} records')\n",
    "    query = make_rel_merge_query(source_target_labels, source_node_key,\n",
    "                                 target_node_key, rel_type, rel_df.columns.copy(), rel_key)\n",
    "    print(f'\\nUsing This Cypher Query:\\n```\\n{query}\\n```\\n')\n",
    "    cumulative_count = 0\n",
    "    for recs in batches(records, batch_size):\n",
    "        res = graph.query(query, params={'recs': recs})\n",
    "        cumulative_count += res[0].get('relLoadedCount')\n",
    "        print(f'Loaded {cumulative_count:,} of {total:,} relationships')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_and_split_txt_data(file_names: List[str]) -> DataFrame:\n",
    "    doc_data_list = []\n",
    "    for file_name in file_names:\n",
    "        with open(file_name) as f:\n",
    "            f10_k = json.load(f)\n",
    "            for item in ['item1', 'item1a', 'item7', 'item7a']:\n",
    "                #split text data\n",
    "                txt = f10_k[item]\n",
    "                split_txts = text_splitter.split_text(txt)\n",
    "                chunk_seq_id = 0\n",
    "                for split_txt in split_txts:\n",
    "                    form_id = file_name[file_name.rindex('/') + 1:file_name.rindex('.')]\n",
    "                    doc_data_list.append({ 'formId': f'{form_id}',\n",
    "                                           'chunkId': f'{form_id}-{item}-chunk{chunk_seq_id:04d}',\n",
    "                                           'cik': f10_k['cik'],\n",
    "                                           'cusip6': f10_k['cusip6'],\n",
    "                                           'source': f10_k['source'],\n",
    "                                           'f10kItem': item,\n",
    "                                           'chunkSeqId': chunk_seq_id,\n",
    "                                           'text': split_txt})\n",
    "                    chunk_seq_id += 1\n",
    "    return pd.DataFrame(doc_data_list)\n",
    "\n",
    "def add_text_embeddings(df):\n",
    "    count = 0\n",
    "    embeddings = []\n",
    "    for docs in batches(df.text, n=100):\n",
    "        count += len(docs)\n",
    "        print(f'Embedded {count} of {df.shape[0]}')\n",
    "        embeddings.extend(embeddings_model.embed_documents(docs))\n",
    "    df['textEmbedding'] = embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Form 10k documents\n",
    "\n",
    "1. iterate through all the files in the directory\n",
    "2. batch load sets of the files\n",
    "3. for each file, load the content and split the text into chunks\n",
    "4. for each chunk, create a graph Node that includes metadata and the chunk text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing 0:20 of 79 ===\n",
      "Loading and splitting Text Files...\n",
      "Performing Text Embedding...\n",
      "Embedded 100 of 2485\n",
      "Embedded 200 of 2485\n",
      "Embedded 300 of 2485\n",
      "Embedded 400 of 2485\n",
      "Embedded 500 of 2485\n",
      "Embedded 600 of 2485\n",
      "Embedded 700 of 2485\n",
      "Embedded 800 of 2485\n",
      "Embedded 900 of 2485\n",
      "Embedded 1000 of 2485\n",
      "Embedded 1100 of 2485\n",
      "Embedded 1200 of 2485\n",
      "Embedded 1300 of 2485\n",
      "Embedded 1400 of 2485\n",
      "Embedded 1500 of 2485\n",
      "Embedded 1600 of 2485\n",
      "Embedded 1700 of 2485\n",
      "Embedded 1800 of 2485\n",
      "Embedded 1900 of 2485\n",
      "Embedded 2000 of 2485\n",
      "Embedded 2100 of 2485\n",
      "Embedded 2200 of 2485\n",
      "Embedded 2300 of 2485\n",
      "Embedded 2400 of 2485\n",
      "Embedded 2485 of 2485\n",
      "Loading Nodes...\n",
      "======  loading Chunk nodes  ======\n",
      "staging 2,485 records\n",
      "\n",
      "Using This Cypher Query:\n",
      "```\n",
      "UNWIND $recs AS rec\n",
      "MERGE(n:Chunk {chunkId: rec.chunkId})\n",
      "SET n.formId = rec.formId, n.cik = rec.cik, n.cusip6 = rec.cusip6, n.source = rec.source, n.f10kItem = rec.f10kItem, n.chunkSeqId = rec.chunkSeqId, n.text = rec.text\n",
      "RETURN count(n) AS nodeLoadedCount\n",
      "```\n",
      "\n",
      "Loaded 1,000 of 2,485 nodes\n",
      "Loaded 2,000 of 2,485 nodes\n",
      "Loaded 2,485 of 2,485 nodes\n",
      "Done Processing 0:20\n",
      "======  loading Document text embeddings ======\n",
      "staging 2,485 records\n",
      "Set 100 of 2,485 text embeddings\n",
      "Set 200 of 2,485 text embeddings\n",
      "Set 300 of 2,485 text embeddings\n",
      "Set 400 of 2,485 text embeddings\n",
      "Set 500 of 2,485 text embeddings\n",
      "Set 600 of 2,485 text embeddings\n",
      "Set 700 of 2,485 text embeddings\n",
      "Set 800 of 2,485 text embeddings\n",
      "Set 900 of 2,485 text embeddings\n",
      "Set 1,000 of 2,485 text embeddings\n",
      "Set 1,100 of 2,485 text embeddings\n",
      "Set 1,200 of 2,485 text embeddings\n",
      "Set 1,300 of 2,485 text embeddings\n",
      "Set 1,400 of 2,485 text embeddings\n",
      "Set 1,500 of 2,485 text embeddings\n",
      "Set 1,600 of 2,485 text embeddings\n",
      "Set 1,700 of 2,485 text embeddings\n",
      "Set 1,800 of 2,485 text embeddings\n",
      "Set 1,900 of 2,485 text embeddings\n",
      "Set 2,000 of 2,485 text embeddings\n",
      "Set 2,100 of 2,485 text embeddings\n",
      "Set 2,200 of 2,485 text embeddings\n",
      "Set 2,300 of 2,485 text embeddings\n",
      "Set 2,400 of 2,485 text embeddings\n",
      "Set 2,485 of 2,485 text embeddings\n",
      "=== Processing 20:40 of 79 ===\n",
      "Loading and splitting Text Files...\n",
      "Performing Text Embedding...\n",
      "Embedded 100 of 2570\n",
      "Embedded 200 of 2570\n",
      "Embedded 300 of 2570\n",
      "Embedded 400 of 2570\n",
      "Embedded 500 of 2570\n",
      "Embedded 600 of 2570\n",
      "Embedded 700 of 2570\n",
      "Embedded 800 of 2570\n",
      "Embedded 900 of 2570\n",
      "Embedded 1000 of 2570\n",
      "Embedded 1100 of 2570\n",
      "Embedded 1200 of 2570\n",
      "Embedded 1300 of 2570\n",
      "Embedded 1400 of 2570\n",
      "Embedded 1500 of 2570\n",
      "Embedded 1600 of 2570\n",
      "Embedded 1700 of 2570\n",
      "Embedded 1800 of 2570\n",
      "Embedded 1900 of 2570\n",
      "Embedded 2000 of 2570\n",
      "Embedded 2100 of 2570\n",
      "Embedded 2200 of 2570\n",
      "Embedded 2300 of 2570\n",
      "Embedded 2400 of 2570\n",
      "Embedded 2500 of 2570\n",
      "Embedded 2570 of 2570\n",
      "Loading Nodes...\n",
      "======  loading Chunk nodes  ======\n",
      "staging 2,570 records\n",
      "\n",
      "Using This Cypher Query:\n",
      "```\n",
      "UNWIND $recs AS rec\n",
      "MERGE(n:Chunk {chunkId: rec.chunkId})\n",
      "SET n.formId = rec.formId, n.cik = rec.cik, n.cusip6 = rec.cusip6, n.source = rec.source, n.f10kItem = rec.f10kItem, n.chunkSeqId = rec.chunkSeqId, n.text = rec.text\n",
      "RETURN count(n) AS nodeLoadedCount\n",
      "```\n",
      "\n",
      "Loaded 1,000 of 2,570 nodes\n",
      "Loaded 2,000 of 2,570 nodes\n",
      "Loaded 2,570 of 2,570 nodes\n",
      "Done Processing 20:40\n",
      "======  loading Document text embeddings ======\n",
      "staging 2,570 records\n",
      "Set 100 of 2,570 text embeddings\n",
      "Set 200 of 2,570 text embeddings\n",
      "Set 300 of 2,570 text embeddings\n",
      "Set 400 of 2,570 text embeddings\n",
      "Set 500 of 2,570 text embeddings\n",
      "Set 600 of 2,570 text embeddings\n",
      "Set 700 of 2,570 text embeddings\n",
      "Set 800 of 2,570 text embeddings\n",
      "Set 900 of 2,570 text embeddings\n",
      "Set 1,000 of 2,570 text embeddings\n",
      "Set 1,100 of 2,570 text embeddings\n",
      "Set 1,200 of 2,570 text embeddings\n",
      "Set 1,300 of 2,570 text embeddings\n",
      "Set 1,400 of 2,570 text embeddings\n",
      "Set 1,500 of 2,570 text embeddings\n",
      "Set 1,600 of 2,570 text embeddings\n",
      "Set 1,700 of 2,570 text embeddings\n",
      "Set 1,800 of 2,570 text embeddings\n",
      "Set 1,900 of 2,570 text embeddings\n",
      "Set 2,000 of 2,570 text embeddings\n",
      "Set 2,100 of 2,570 text embeddings\n",
      "Set 2,200 of 2,570 text embeddings\n",
      "Set 2,300 of 2,570 text embeddings\n",
      "Set 2,400 of 2,570 text embeddings\n",
      "Set 2,500 of 2,570 text embeddings\n",
      "Set 2,570 of 2,570 text embeddings\n",
      "=== Processing 40:60 of 79 ===\n",
      "Loading and splitting Text Files...\n",
      "Performing Text Embedding...\n",
      "Embedded 100 of 1977\n",
      "Embedded 200 of 1977\n",
      "Embedded 300 of 1977\n",
      "Embedded 400 of 1977\n",
      "Embedded 500 of 1977\n",
      "Embedded 600 of 1977\n",
      "Embedded 700 of 1977\n",
      "Embedded 800 of 1977\n",
      "Embedded 900 of 1977\n",
      "Embedded 1000 of 1977\n",
      "Embedded 1100 of 1977\n",
      "Embedded 1200 of 1977\n",
      "Embedded 1300 of 1977\n",
      "Embedded 1400 of 1977\n",
      "Embedded 1500 of 1977\n",
      "Embedded 1600 of 1977\n",
      "Embedded 1700 of 1977\n",
      "Embedded 1800 of 1977\n",
      "Embedded 1900 of 1977\n",
      "Embedded 1977 of 1977\n",
      "Loading Nodes...\n",
      "======  loading Chunk nodes  ======\n",
      "staging 1,977 records\n",
      "\n",
      "Using This Cypher Query:\n",
      "```\n",
      "UNWIND $recs AS rec\n",
      "MERGE(n:Chunk {chunkId: rec.chunkId})\n",
      "SET n.formId = rec.formId, n.cik = rec.cik, n.cusip6 = rec.cusip6, n.source = rec.source, n.f10kItem = rec.f10kItem, n.chunkSeqId = rec.chunkSeqId, n.text = rec.text\n",
      "RETURN count(n) AS nodeLoadedCount\n",
      "```\n",
      "\n",
      "Loaded 1,000 of 1,977 nodes\n",
      "Loaded 1,977 of 1,977 nodes\n",
      "Done Processing 40:60\n",
      "======  loading Document text embeddings ======\n",
      "staging 1,977 records\n",
      "Set 100 of 1,977 text embeddings\n",
      "Set 200 of 1,977 text embeddings\n",
      "Set 300 of 1,977 text embeddings\n",
      "Set 400 of 1,977 text embeddings\n",
      "Set 500 of 1,977 text embeddings\n",
      "Set 600 of 1,977 text embeddings\n",
      "Set 700 of 1,977 text embeddings\n",
      "Set 800 of 1,977 text embeddings\n",
      "Set 900 of 1,977 text embeddings\n",
      "Set 1,000 of 1,977 text embeddings\n",
      "Set 1,100 of 1,977 text embeddings\n",
      "Set 1,200 of 1,977 text embeddings\n",
      "Set 1,300 of 1,977 text embeddings\n",
      "Set 1,400 of 1,977 text embeddings\n",
      "Set 1,500 of 1,977 text embeddings\n",
      "Set 1,600 of 1,977 text embeddings\n",
      "Set 1,700 of 1,977 text embeddings\n",
      "Set 1,800 of 1,977 text embeddings\n",
      "Set 1,900 of 1,977 text embeddings\n",
      "Set 1,977 of 1,977 text embeddings\n",
      "=== Processing 60:79 of 79 ===\n",
      "Loading and splitting Text Files...\n",
      "Performing Text Embedding...\n",
      "Embedded 100 of 2687\n",
      "Embedded 200 of 2687\n",
      "Embedded 300 of 2687\n",
      "Embedded 400 of 2687\n",
      "Embedded 500 of 2687\n",
      "Embedded 600 of 2687\n",
      "Embedded 700 of 2687\n",
      "Embedded 800 of 2687\n",
      "Embedded 900 of 2687\n",
      "Embedded 1000 of 2687\n",
      "Embedded 1100 of 2687\n",
      "Embedded 1200 of 2687\n",
      "Embedded 1300 of 2687\n",
      "Embedded 1400 of 2687\n",
      "Embedded 1500 of 2687\n",
      "Embedded 1600 of 2687\n",
      "Embedded 1700 of 2687\n",
      "Embedded 1800 of 2687\n",
      "Embedded 1900 of 2687\n",
      "Embedded 2000 of 2687\n",
      "Embedded 2100 of 2687\n",
      "Embedded 2200 of 2687\n",
      "Embedded 2300 of 2687\n",
      "Embedded 2400 of 2687\n",
      "Embedded 2500 of 2687\n",
      "Embedded 2600 of 2687\n",
      "Embedded 2687 of 2687\n",
      "Loading Nodes...\n",
      "======  loading Chunk nodes  ======\n",
      "staging 2,687 records\n",
      "\n",
      "Using This Cypher Query:\n",
      "```\n",
      "UNWIND $recs AS rec\n",
      "MERGE(n:Chunk {chunkId: rec.chunkId})\n",
      "SET n.formId = rec.formId, n.cik = rec.cik, n.cusip6 = rec.cusip6, n.source = rec.source, n.f10kItem = rec.f10kItem, n.chunkSeqId = rec.chunkSeqId, n.text = rec.text\n",
      "RETURN count(n) AS nodeLoadedCount\n",
      "```\n",
      "\n",
      "Loaded 1,000 of 2,687 nodes\n",
      "Loaded 2,000 of 2,687 nodes\n",
      "Loaded 2,687 of 2,687 nodes\n",
      "Done Processing 60:79\n",
      "======  loading Document text embeddings ======\n",
      "staging 2,687 records\n",
      "Set 100 of 2,687 text embeddings\n",
      "Set 200 of 2,687 text embeddings\n",
      "Set 300 of 2,687 text embeddings\n",
      "Set 400 of 2,687 text embeddings\n",
      "Set 500 of 2,687 text embeddings\n",
      "Set 600 of 2,687 text embeddings\n",
      "Set 700 of 2,687 text embeddings\n",
      "Set 800 of 2,687 text embeddings\n",
      "Set 900 of 2,687 text embeddings\n",
      "Set 1,000 of 2,687 text embeddings\n",
      "Set 1,100 of 2,687 text embeddings\n",
      "Set 1,200 of 2,687 text embeddings\n",
      "Set 1,300 of 2,687 text embeddings\n",
      "Set 1,400 of 2,687 text embeddings\n",
      "Set 1,500 of 2,687 text embeddings\n",
      "Set 1,600 of 2,687 text embeddings\n",
      "Set 1,700 of 2,687 text embeddings\n",
      "Set 1,800 of 2,687 text embeddings\n",
      "Set 1,900 of 2,687 text embeddings\n",
      "Set 2,000 of 2,687 text embeddings\n",
      "Set 2,100 of 2,687 text embeddings\n",
      "Set 2,200 of 2,687 text embeddings\n",
      "Set 2,300 of 2,687 text embeddings\n",
      "Set 2,400 of 2,687 text embeddings\n",
      "Set 2,500 of 2,687 text embeddings\n",
      "Set 2,600 of 2,687 text embeddings\n",
      "Set 2,687 of 2,687 text embeddings\n",
      "CPU times: user 35 s, sys: 1.23 s, total: 36.2 s\n",
      "Wall time: 3min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "all_file_names = ['../source-data-pull/form10k/data/form10k-clean/' + x for x in os.listdir('../source-data-pull/form10k/data/form10k-clean/')]\n",
    "counter = 0\n",
    "for file_names in batches(all_file_names, 20):\n",
    "    counter += len(file_names)\n",
    "    print(f'=== Processing {counter-len(file_names)}:{counter} of {len(all_file_names)} ===')\n",
    "    # get and split text data\n",
    "    print('Loading and splitting Text Files...')\n",
    "    doc_df = get_and_split_txt_data(file_names)\n",
    "    # perform text embedding\n",
    "    print('Performing Text Embedding...')\n",
    "    add_text_embeddings(doc_df)\n",
    "    #load nodes\n",
    "    print('Loading Nodes...')\n",
    "    load_nodes(kg, doc_df.drop(columns='textEmbedding'), 'chunkId', 'Chunk')\n",
    "    print(f'Done Processing {counter-len(file_names)}:{counter}')\n",
    "\n",
    "    # Merge text embeddings using set vector property\n",
    "    records = doc_df[['chunkId', 'textEmbedding']].to_dict('records')\n",
    "    print(f'======  loading Document text embeddings ======')\n",
    "    total = len(records)\n",
    "    print(f'staging {total:,} records')\n",
    "    cumulative_count = 0\n",
    "    for recs in batches(records, n=100):\n",
    "        res = kg.query('''\n",
    "        UNWIND $recs AS rec\n",
    "        MATCH(n:Chunk {chunkId: rec.chunkId})\n",
    "        CALL db.create.setNodeVectorProperty(n, \"textEmbedding\", rec.textEmbedding)\n",
    "        RETURN count(n) AS propertySetCount\n",
    "        ''', params={'recs': recs})\n",
    "        cumulative_count += res[0].get('propertySetCount')\n",
    "        print(f'Set {cumulative_count:,} of {total:,} text embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Who makes hydraulic and mechanical tools?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'result': \"The Company's critical raw material is steel. Out of Brazil, the Company sources three basic types of steel which are carbon steel, high speed steel and carbide cylinders. The Company has a number of long-term suppliers in Europe, Asia and Brazil and its sourcing mix is distributed according to the pricing including exchange rates. The U.S. sources steel, and small amounts of aluminum and brass through distributors. None of these suppliers accounts for more than 5% of the Company's purchases\\nFor over 140 years, the Company has been a recognized leader in providing measurement and cutting solutions to industry. Measurement tools consist of precision instruments such as micrometers, vernier calipers, height distributors, depth gages, electronic gages, dial indicators, steel rules, combination squares, custom, non-contact gaging such as vision, optical and laser measurement systems. The Company believes advanced, non-contact systems with easy-to use software will be attractive to industry to reduce measurement and inspection time and are ideal for quality assurance, inspection labs, manufacturing and research facilities. Skilled personnel, superior products, manufacturing expertise, innovation and unmatched service has earned the Company its reputation as the “Best in Class” provider of measuring application solutions for industry.\\nAs one of the premier industrial brands, the Company continues to be focused on every touch point with its customers. To that end, the Company now offers modern, easy-to-use interfaces for distributors and end-users including interactive catalogs and several online applications.\"},\n",
       " {'result': 'When used herein, the terms “we,” “us,” “our,” “Enerpac,” and the “Company” refer to Enerpac Tool Group Corp. and its subsidiaries. Reference to fiscal years, such as “fiscal 2023,” are to the fiscal year ending on August 31 of the specified year.\\nPART I\\nItem\\xa0\\xa01.\\xa0\\xa0\\xa0\\xa0\\nBusiness\\nGeneral\\nEnerpac Tool Group Corp. is a premier industrial tools, services, technology and solutions company serving a broad and diverse set of customers in more than 100 countries. Enerpac Tool Group\\'s businesses are global leaders of high pressure hydraulic tools, controlled force products and solutions for precise positioning of heavy loads that help customers around the world safely, reliably and efficiently tackle some of the most challenging, complex, and often hazardous jobs. The Company was founded in 1910 and is headquartered in Menomonee Falls, Wisconsin. The Company has one reportable segment, the Industrial Tools & Services Segment (\"IT&S\"). The IT&S segment is primarily engaged in the design, manufacture and distribution of branded hydraulic and mechanical tools and in providing services and tool rental to the industrial, maintenance, infrastructure, oil & gas, alternative energy and other markets. Financial information related to the Company\\'s reportable segment is included in \\nNote 15, \"Business Segment, Geographic and Customer Information\"\\n in the notes to the consolidated financial statements. Our businesses provide an array of products and services across multiple markets and geographies, which results in significant diversification. The IT&S segment and the Company are well-positioned to drive shareholder value through a sustainable business strategy built on well-established brands, broad global distribution and end markets, with a clear focus on the core tools and services business, and disciplined capital deployment.'},\n",
       " {'result': 'The Company is one of the largest producers of mechanics’ hand measuring tools and precision instruments. In the United States, there are three major foreign competitors and numerous small companies in the field. As a result, the industry is highly competitive. During fiscal 2023, there were no material changes in the Company’s competitive position. The Company’s products for the building trades, such as tape measures and levels, are under constant margin pressure due to a channel shift to large national home and hardware retailers. The Company has responded to such challenges by expanding its manufacturing operations in China.\\xa0Certain large customers also offer their own private labels “own brand” that compete with Starrett branded products. These products are often sourced directly from low cost countries.\\nSaw products encounter competition from several domestic and international sources. The Company’s competitive position varies by market and country. Continued research and development, new patented products and processes, strategic acquisitions and investments and strong customer support have enabled the Company to compete successfully in both general and performance applications.\\nForeign Operations\\nThe operations of the Company’s foreign subsidiaries are consolidated in its financial statements. The subsidiaries located in Brazil and China are actively engaged in the manufacturing and distribution of precision measuring tools, saw blades, optical and vision measuring equipment and hand tools. Subsidiaries in Scotland, Canada, Australia, New Zealand, and Mexico are \\n7'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector search using our utility function\n",
    "neo4j_vector_search(kg, embeddings_model, VECTOR_INDEX_NAME, question, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Score:  0.8979572653770447\n",
      "\n",
      "text: The Company's critical raw material is steel. Out of Brazil, the Company sources three basic types of steel which are carbon steel, high speed steel and carbide cylinders. The Company has a number of long-term suppliers in Europe, Asia and Brazil and its sourcing mix is distributed according to the pricing including exchange rates. The U.S. sources steel, and small amounts of aluminum and brass through distributors. None of these suppliers accounts for more than 5% of the Company's purchases\n",
      "For over 140 years, the Company has been a recognized leader in providing measurement and cutting solutions to industry. Measurement tools consist of precision instruments such as micrometers, vernier calipers, height distributors, depth gages, electronic gages, dial indicators, steel rules, combination squares, custom, non-contact gaging such as vision, optical and laser measurement systems. The Company believes advanced, non-contact systems with easy-to use software will be attractive to industry to reduce measurement and inspection time and are ideal for quality assurance, inspection labs, manufacturing and research facilities. Skilled personnel, superior products, manufacturing expertise, innovation and unmatched service has earned the Company its reputation as the “Best in Class” provider of measuring application solutions for industry.\n",
      "As one of the premier industrial brands, the Company continues to be focused on every touch point with its customers. To that end, the Company now offers modern, easy-to-use interfaces for distributors and end-users including interactive catalogs and several online applications.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.8943517804145813\n",
      "\n",
      "text: When used herein, the terms “we,” “us,” “our,” “Enerpac,” and the “Company” refer to Enerpac Tool Group Corp. and its subsidiaries. Reference to fiscal years, such as “fiscal 2023,” are to the fiscal year ending on August 31 of the specified year.\n",
      "PART I\n",
      "Item  1.    \n",
      "Business\n",
      "General\n",
      "Enerpac Tool Group Corp. is a premier industrial tools, services, technology and solutions company serving a broad and diverse set of customers in more than 100 countries. Enerpac Tool Group's businesses are global leaders of high pressure hydraulic tools, controlled force products and solutions for precise positioning of heavy loads that help customers around the world safely, reliably and efficiently tackle some of the most challenging, complex, and often hazardous jobs. The Company was founded in 1910 and is headquartered in Menomonee Falls, Wisconsin. The Company has one reportable segment, the Industrial Tools & Services Segment (\"IT&S\"). The IT&S segment is primarily engaged in the design, manufacture and distribution of branded hydraulic and mechanical tools and in providing services and tool rental to the industrial, maintenance, infrastructure, oil & gas, alternative energy and other markets. Financial information related to the Company's reportable segment is included in \n",
      "Note 15, \"Business Segment, Geographic and Customer Information\"\n",
      " in the notes to the consolidated financial statements. Our businesses provide an array of products and services across multiple markets and geographies, which results in significant diversification. The IT&S segment and the Company are well-positioned to drive shareholder value through a sustainable business strategy built on well-established brands, broad global distribution and end markets, with a clear focus on the core tools and services business, and disciplined capital deployment.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.8915809392929077\n",
      "\n",
      "text: The Company is one of the largest producers of mechanics’ hand measuring tools and precision instruments. In the United States, there are three major foreign competitors and numerous small companies in the field. As a result, the industry is highly competitive. During fiscal 2023, there were no material changes in the Company’s competitive position. The Company’s products for the building trades, such as tape measures and levels, are under constant margin pressure due to a channel shift to large national home and hardware retailers. The Company has responded to such challenges by expanding its manufacturing operations in China. Certain large customers also offer their own private labels “own brand” that compete with Starrett branded products. These products are often sourced directly from low cost countries.\n",
      "Saw products encounter competition from several domestic and international sources. The Company’s competitive position varies by market and country. Continued research and development, new patented products and processes, strategic acquisitions and investments and strong customer support have enabled the Company to compete successfully in both general and performance applications.\n",
      "Foreign Operations\n",
      "The operations of the Company’s foreign subsidiaries are consolidated in its financial statements. The subsidiaries located in Brazil and China are actively engaged in the manufacturing and distribution of precision measuring tools, saw blades, optical and vision measuring equipment and hand tools. Subsidiaries in Scotland, Canada, Australia, New Zealand, and Mexico are \n",
      "7\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Vector search using the langchain vector store\n",
    "docs_with_score = vector_store.similarity_search_with_score(question, k=3)\n",
    "\n",
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"\\ntext: The Company's critical raw material is steel. Out of Brazil, the Company sources three basic types of steel which are carbon steel, high speed steel and carbide cylinders. The Company has a number of long-term suppliers in Europe, Asia and Brazil and its sourcing mix is distributed according to the pricing including exchange rates. The U.S. sources steel, and small amounts of aluminum and brass through distributors. None of these suppliers accounts for more than 5% of the Company's purchases\\nFor over 140 years, the Company has been a recognized leader in providing measurement and cutting solutions to industry. Measurement tools consist of precision instruments such as micrometers, vernier calipers, height distributors, depth gages, electronic gages, dial indicators, steel rules, combination squares, custom, non-contact gaging such as vision, optical and laser measurement systems. The Company believes advanced, non-contact systems with easy-to use software will be attractive to industry to reduce measurement and inspection time and are ideal for quality assurance, inspection labs, manufacturing and research facilities. Skilled personnel, superior products, manufacturing expertise, innovation and unmatched service has earned the Company its reputation as the “Best in Class” provider of measuring application solutions for industry.\\nAs one of the premier industrial brands, the Company continues to be focused on every touch point with its customers. To that end, the Company now offers modern, easy-to-use interfaces for distributors and end-users including interactive catalogs and several online applications.\", metadata={'cik': '93676', 'source': 'https://www.sec.gov/Archives/edgar/data/93676/000009367623000035/0000093676-23-000035-index.htm', 'formId': '0000093676-23-000035', 'f10kItem': 'item7', 'chunkId': '0000093676-23-000035-item7-chunk0005', 'cusip6': '855668', 'chunkSeqId': 5})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector search using the langchain retriever over the Neo4j vector store\n",
    "retriever.get_relevant_documents(question)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Enerpac Tool Group Corp. is the company that makes hydraulic and mechanical tools.\\n',\n",
       " 'sources': 'https://www.sec.gov/Archives/edgar/data/6955/000000695523000034/0000006955-23-000034-index.htm'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\n",
    "    {\"question\": question},\n",
    "    return_only_outputs=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
